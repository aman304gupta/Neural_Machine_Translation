{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution() ## enabling eager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if it is executing eagerly\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    ## unicodedata.normalize -- is normalizing the sentence to NFD form\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## w -- is single french sent / english sent\n",
    "def preprocess_sentence(w):\n",
    "    \n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    w = re.sub(r\"([?.!,()])\",r\"\\1\",w)\n",
    "    w = re.sub('\\s{2,}',' ',w)\n",
    "    \n",
    "    w = re.sub(r\"[^a-zA-z.?!,()]+\",\" \",w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    w = '<start>' + w + '<end>'\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fxn - create dataset - \n",
    "#i/p -> filename, how many pairs to be extracted\n",
    "# o/p -> pairs of 2 languages\n",
    "\n",
    "def create_dataset(filename, num_examples):\n",
    "    \n",
    "    lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
    "    ## lines[0] = 'English word    French word'\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(pair) for pair in line.split('\\t')] for line in lines[:num_examples]]\n",
    "    ## word_pairs[0] = [['english word',\"French word\"]]\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class creates dixtionary mapping -- word to index && index to word\n",
    "class LanguageIndex():\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {} ## dict -- word 2 index\n",
    "        self.idx2word = {} ## dict -- index to word\n",
    "        self.vocab = set() ## creating a set for vocab\n",
    "        ## caall this class's method\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        \n",
    "        for phrase in self.lang: ## dividing the phrases of language\n",
    "            self.vocab.update(phrase.split(\" \")) ## adding to set\n",
    "            \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0 ## set first word's index as zero\n",
    "        \n",
    "        for index, word in enumerate(self.vocab): ## since enumerate therefore index\n",
    "            self.word2idx[word] = index + 1\n",
    "            \n",
    "        for word, index in self.word2idx.items(): ## iterating over dictionary\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding max length\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    \n",
    "    ## path -- filename, num_examples -- examples to be included\n",
    "    pairs = create_dataset(path, num_examples) # word pairs created --pairs[0] = [['english word','french word']]\n",
    "        \n",
    "    ## calling init fxn of class and thus creating object\n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    ## spanish sentences -- each word converted to it's index\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    ## english sentences -- each word converted to it's index\n",
    "    target_tensor = [[targ_lang.word2idx[e] for e in en.split(' ')] for en,sp in pairs]\n",
    "    \n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    ## psot padding both tensors\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                maxlen=max_length_inp,\n",
    "                                                               padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
    "                                                                maxlen=max_length_tar,\n",
    "                                                                padding='post')\n",
    "    \n",
    "    ## returning padded tensors' && i/p o/p lang objects, and aximum lengthh of oth tensors\n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar= load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000 24000 6000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tensor_train),len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE/ BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "## creating dataset using data API\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "## Settign batch\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True) ## i.e drop the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
